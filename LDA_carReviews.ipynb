{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9689925-91c2-4f8c-9b7c-dc452e89b3d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec05c0f-0dff-4e62-a48e-068b46c6d414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4ac3d-d0f0-41a7-b37f-4bcbc756ad50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "\n",
    "filesArray = os.listdir(f'{PATH}/archive')\n",
    "csvFiles = [file for file in filesArray if file.endswith(\".csv\")]\n",
    "csvFiles.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada1be9-d451-408c-966a-7e487c6c6274",
   "metadata": {},
   "source": [
    "#### combine to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4774171-9800-4d5c-adbc-7ed426507e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine into one data frame\n",
    "data = []\n",
    "for i in csvFiles:\n",
    "    data.append(pd.read_csv(f'{PATH}/archive/{i}'))\n",
    "\n",
    "reviewsRaw = pd.concat(data, ignore_index=True)\n",
    "\n",
    "columnNames = {'Date of Exp' : 'dateExp', 'Star Rating' : 'starRating', 'Reviews': 'reviews'}\n",
    "reviewsRaw = reviewsRaw.rename(columns=columnNames)\n",
    "\n",
    "reviewsRaw['dateExp'] = pd.to_datetime(reviewsRaw['dateExp'], format = 'mixed')\n",
    "reviewsRaw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d8c9c-2743-4c98-bc91-6fce42411b19",
   "metadata": {},
   "source": [
    "### maybe include date filter here?\n",
    "\n",
    "treat onedate = onedocument\n",
    "\n",
    "topics modelling will be per day (or according to desired range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf23cb3-5865-474d-a6e0-abe3955ccbf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to get only the rows from 2023\n",
    "reviews = reviewsRaw.loc[reviewsRaw['dateExp'].dt.year >= 2022].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde07456-df65-4e9a-a852-724164702607",
   "metadata": {},
   "source": [
    "> By using the .loc accessor and the .copy() method, you explicitly indicate that you want to modify a specific subset of the DataFrame, avoiding the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4d007-b38a-4114-8eff-ce6a3a91d615",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataPreprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f2516-e2e7-4b9d-9160-c83b0be4fbb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### recode starRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297aec50-6753-427c-99d6-d1d6fad5aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['starRatingRecode'] = reviews['starRating'].apply(lambda x: 'positive' if x >=4 else ('neutral' if x==3 else 'negative'))\n",
    "reviews.head()\n",
    "#print(reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3178b-44f8-4b3f-b639-e0c21cf6360b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### remove puctuations & convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923d69d-7dcd-4a56-8876-20089a49729d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def lowerCase_removePunc(dataframeName, columnName):\n",
    "    dataframeName[columnName] = dataframeName['reviews'].map(lambda text: re.sub(r'[,\\.!?]', '', text))\n",
    "    dataframeName[columnName] = dataframeName[columnName].map(lambda text: text.lower())    \n",
    "    return dataframeName\n",
    "\n",
    "\n",
    "reviews = lowerCase_removePunc(reviews, 'processedText')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79ed08-cf53-4267-baf3-6a51ecb07e28",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309c3a0-4fac-498b-b286-e7bcc099f2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = (reviews['processedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712f3e5-7a4a-441b-8a19-a7293f5b2839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "processedText = content.apply(tokenize)\n",
    "processedText.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda494c5-218e-4b5e-9f8e-61412a30b7e1",
   "metadata": {},
   "source": [
    "#### remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace5901-5b4e-48c8-bcc6-115b897c3cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords  = stopwords.words('english')\n",
    "#stopwords\n",
    "\n",
    "def removeStopwords(tokenizedText):\n",
    "    filteredTokens = [token for token in tokenizedText if token.lower() not in stopwords]\n",
    "    return filteredTokens\n",
    "\n",
    "processedText = processedText.apply(removeStopwords)\n",
    "#processedText.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b1ddc-75ed-4d62-bb27-70da11b23419",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f7dd5-0bdd-4505-9800-fe5839528659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "longString = ' '.join(processedText.apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a3931-bbb4-4558-8a9e-441f7de8bcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=300,\n",
    "    max_font_size=50,\n",
    "    random_state=42\n",
    ").generate(longString)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7b986-4eec-4f7b-9dc4-67ed23039e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the wordcloud image\n",
    "wordcloud_image_path = 'wordcloud.png'  # Specify the path and filename\n",
    "wordcloud.to_file(wordcloud_image_path)\n",
    "print(f\"Wordcloud saved as {wordcloud_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa107d59-935c-4ed2-aa71-8dd72be2646f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d87e0-881c-4fec-b983-0f5a98a3bbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(tokenizedText):\n",
    "    filteredTokens = [lemmatizer.lemmatize(token) for token in tokenizedText]\n",
    "    return filteredTokens\n",
    "\n",
    "#processedText = processedText.apply(lemmatize)\n",
    "#processedText.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3814d97b-72d8-4f2d-b3c5-acd7f0097aba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modelling using LDA-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f46f2-7499-4fa6-b6e7-28561c8cd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Prepare the corpus\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess \n",
    "# simple_preprocess:\n",
    "# tokenization, lowercasing,\n",
    "# filtering removes tokens that are too short (less than 3 characters) or too long (more than 15 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556bd41-4f21-44c0-81e3-6222123e65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = (reviews['reviews'])\n",
    "corpus = []\n",
    "\n",
    "def preprocess_corpus(data):\n",
    "    processed_corpus = data.apply(lambda x: simple_preprocess(x))\n",
    "    return processed_corpus\n",
    "\n",
    "preprocessed_corpus = preprocess_corpus(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14035f-6414-4225-898e-614b7e59b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4c36e-d15e-4b54-ba27-9b9cbce3cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_corpus[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73eadcc-7f52-4cd2-b8ea-3a5ebd6946f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Creating dictionary\n",
    "from nltk import bigrams\n",
    "from gensim import corpora, models\n",
    "\n",
    "def create_bigrams(corpus):\n",
    "    # Create a list to hold the bigram models\n",
    "    corpus_bigrams = []\n",
    "\n",
    "    # Create bigrams for each document in the corpus\n",
    "    for doc in corpus:\n",
    "        doc_bigrams = list(bigrams(doc))\n",
    "        doc_bigrams = [' '.join(bigram) for bigram in doc_bigrams]  # Convert bigrams to strings\n",
    "        corpus_bigrams.append(doc_bigrams)\n",
    "\n",
    "    return corpus_bigrams\n",
    "\n",
    "def create_dict_tfidf(corpus):\n",
    "    # create dict using the preprocessed corpus\n",
    "    dictionary = corpora.Dictionary(corpus)\n",
    "    \n",
    "    # create bag-of-words representation of the corpus\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    \n",
    "    # create tf-idf model and convert the bow vector to tfidf vectors\n",
    "    tfidf_model = models.TfidfModel(bow_corpus)\n",
    "    tfidf_corpus = tfidf_model[bow_corpus]\n",
    "    \n",
    "    return dictionary, tfidf_corpus\n",
    "\n",
    "# dictionary, tfidf_corpus = create_dict_tfidf(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aeb043-c25d-4fdd-84b5-9597e9acf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing steps to the corpus\n",
    "preprocessed_corpus = preprocess_corpus(content)\n",
    "preprocessed_corpus = preprocessed_corpus.apply(removeStopwords)\n",
    "preprocessed_corpus = preprocessed_corpus.apply(lemmatize)\n",
    "\n",
    "# Create the bigrams in the corpus\n",
    "corpus_bigrams = create_bigrams(preprocessed_corpus)\n",
    "\n",
    "# Create the dictionary and TF-IDF corpus with bigrams\n",
    "dictionary, tfidf_corpus = create_dict_tfidf(corpus_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf81139-ef49-4c91-b05e-9a6437aa4270",
   "metadata": {},
   "source": [
    "> By incorporating the TF-IDF transformation, you can assign higher weights to terms that are important in a particular document while downweighting terms that are common across multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ceccb4-fc6e-43ba-ae6b-d4be1fbb483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f825b-580b-4469-8d8a-c2ac152cb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4ac1a-ba4f-4071-bf43-f292f6882b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for doc in tfidf_corpus:\n",
    "#    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7926ee-5b0a-42e0-a2bd-c813b4d77d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build LDA model\n",
    "\n",
    "def train_lda_model(corpus, num_topics):\n",
    "    # Train lda model on tf-idf corpus\n",
    "    lda_model = models.LdaModel(corpus=corpus,\n",
    "                                num_topics=num_topics,\n",
    "                                id2word=dictionary,\n",
    "                                passes=20)\n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10a14b-2193-4d5a-98e4-7299037fe569",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_topics = 5\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model_gensim = train_lda_model(tfidf_corpus, num_topics)\n",
    "\n",
    "# Print the topics and their corresponding keywords\n",
    "for topic_id, topic_words in lda_model_gensim.print_topics():\n",
    "    print(f\"Topic #{topic_id+1}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8c915-5433-4861-95eb-895973b14923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze topics\n",
    "\n",
    "# Get the top keywords for each topic\n",
    "topics = lda_model_gensim.show_topics(num_topics=num_topics, num_words=20)\n",
    "\n",
    "# Assign documents to topics\n",
    "document_topics = [lda_model_gensim.get_document_topics(doc) for doc in tfidf_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc186d7-a741-4c76-ade4-5dafa360bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import webbrowser\n",
    "\n",
    "# Visualize the topics\n",
    "vis_data = gensimvis.prepare(lda_model_gensim, tfidf_corpus, dictionary)\n",
    "\n",
    "# Convert the document-topic assignments to a format suitable for visualization\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model_gensim, tfidf_corpus, dictionary)\n",
    "\n",
    "pyLDAvis.save_html(vis_data, 'lda_visualization2.html')\n",
    "webbrowser.open('lda_visualization.html', new=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb60ffa-e990-4806-ae39-15ca16aaaad6",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17178a5d-71c1-4aa0-a2b0-f17c1df51e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model = CoherenceModel(model=lda_model_gensim, \n",
    "                                 texts=corpus, \n",
    "                                 dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherence Score:\", coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48f5b0-93ea-46d0-a7c6-e0691ebdfdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953276d-1ccf-4324-aeb3-8f10bca311b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges (bigrams) to the graph\n",
    "for doc in corpus_bigrams:\n",
    "    for bigram in doc:\n",
    "        G.add_edge(bigram[0], bigram[1], weight=1)\n",
    "\n",
    "# Plot the network graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(G)  # Layout algorithm for graph visualization\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=12, edge_color='gray')\n",
    "labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.title('Bigram Network Graph')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f147529-982a-4144-ab9e-845267b8f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'_'.join(corpus_bigrams[0][0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4890276-cc13-47bb-a47d-fea491420363",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [ '_'.join(bigram.split(' ')) for bigram in corpus_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026789de-317b-4ef8-9401-b542fb929675",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = ' '.join(['_'.join(bigram.split(' ')) for doc_bigrams in corpus_bigrams for bigram in doc_bigrams])\n",
    "long_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f6ced-e184-4c23-84e9-cf721d8ef55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_bigrams = ['_'.join(bigram.split(' ')) for doc_bigrams in corpus_bigrams for bigram in doc_bigrams]\n",
    "joined_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f215c-a806-4aff-8697-ac134786b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddc3a8-0b7c-4ef7-9087-a696cb944ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the salient terms for each topic\n",
    "topic_terms = lda_model_gensim.show_topics(num_topics=num_topics, num_words=20, formatted=False)\n",
    "\n",
    "# Create a summary for each topic using the salient terms\n",
    "topic_summaries = []\n",
    "\n",
    "for topic in topic_terms:\n",
    "    terms = [term for term, _ in topic[1]]\n",
    "    summary = \" \".join(terms)\n",
    "    topic_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8f0e9-7a59-4c11-ba4c-e86047b3e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2913e0-0ffa-4bed-bf90-20b3f02c096f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
